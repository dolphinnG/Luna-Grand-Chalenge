{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nodule_classifier.dsetsG import CTScan, create_df_candidates_info\n",
    "from util.utilG import getCacheHandle, unzipped_path, xyz2irc\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# log = logging.getLogger('ggggg')\n",
    "# # log.setLevel(logging.WARN)\n",
    "# # log.setLevel(logging.INFO)\n",
    "# log.setLevel(logging.DEBUG)\n",
    "\n",
    "disk_cache = getCacheHandle('segmentation')\n",
    "\n",
    "\n",
    "def get_candidate_info(series_uid):\n",
    "    df_candidates = create_df_candidates_info()\n",
    "    return df_candidates.loc[series_uid].values.tolist()\n",
    "\n",
    "def get_series_uids_list():\n",
    "    df_candidates = create_df_candidates_info()\n",
    "    return df_candidates.index.unique().tolist()\n",
    "\n",
    "class CTScan_seg(CTScan):\n",
    "    def __init__(self, series_uid) -> None:\n",
    "        self.seriesuid = series_uid\n",
    "        path_mhdfile = glob.glob(unzipped_path + 'subset*/subset*/{}.mhd'.format(series_uid))[0]\n",
    "        ct_img = sitk.ReadImage(path_mhdfile) #contain metadata getters\n",
    "        ct_img_arr = sitk.GetArrayFromImage(ct_img).astype(np.float32)\n",
    "        self.ct_img_arr = ct_img_arr\n",
    "         # no longer clip hu [-1000, 1000] here \n",
    "         # because we want to keep the original values of the CT scan\n",
    "        \n",
    "        self.origin_xyz = np.array(ct_img.GetOrigin())\n",
    "        self.vxSize_xyz = np.array(ct_img.GetSpacing())\n",
    "        self.direction_matrix = np.array(ct_img.GetDirection()).reshape(3, 3)\n",
    "        \n",
    "        candidateInfo_list = get_candidate_info(series_uid) # for 1 ct uid only\n",
    "        \n",
    "        \"\"\"idea: build a mask of the positive candidates, \n",
    "        then get the indexes of the mask by summing over the axis 1 and 2, \n",
    "        since the mask is 1 where the pixel is positive, and 0 where the pixel is negative.\n",
    "        We are only aiming to segment the positive candidates, \n",
    "        so we only need the indexes of the positive candidates.\n",
    "        \"\"\"\n",
    "        self.positiveInfo_list = [candidate_tup for candidate_tup \n",
    "                                  in candidateInfo_list if candidate_tup[1]] # idx 1 is isNodule\n",
    "        self.positive_mask = self.buildAnnotationMask(self.positiveInfo_list)\n",
    "        self.positive_indexes = (self.positive_mask.sum(axis=(1,2))\n",
    "                                 .nonzero()[0].tolist()) # get the Is in IRC of the positive masks\n",
    "\n",
    "    def buildAnnotationMask(self, positiveInfo_list, threshold_hu = -700): \n",
    "        #need to fix the bug of wrapping around at 0 index of the tensor\n",
    "        mask_arr = np.zeros_like(self.ct_img_arr, dtype=bool)\n",
    "        \"\"\"So, a HU value of -700 is less dense than water, air, and lung tissue. It's much less dense than bone or other soft tissues. In the context of a lung CT scan, a HU value of -700 would likely correspond to very low-density tissue or possibly an area of disease or damage.\"\"\"\n",
    "        for candidateInfo_tup in positiveInfo_list: \n",
    "            #loop over all the positive candidates of a single chosen ct scan, \n",
    "            # the positiveInfo_list is filtered to only \n",
    "            # contain candidates of a single ct series uid \n",
    "            center_irc = xyz2irc(\n",
    "                candidateInfo_tup[0], # idx 0 is xyzCoord\n",
    "                self.origin_xyz,\n",
    "                self.vxSize_xyz,\n",
    "                self.direction_matrix,\n",
    "            )\n",
    "            ci = int(center_irc[0])\n",
    "            cr = int(center_irc[1])\n",
    "            cc = int(center_irc[2])\n",
    "            # from the center of the candidate, \n",
    "            # we expand the bounding box until we reach the threshold_hu\n",
    "            # we do this for all 3 axes, and we stop expanding an axis once one direction \n",
    "            # of a given axis reaches the threshold_hu, and continue to the next axis\n",
    "            index_step = 2\n",
    "            try: # we loop until we touch the threshold_hu\n",
    "                while self.ct_img_arr[ci + index_step, cr, cc] > threshold_hu and \\\n",
    "                        self.ct_img_arr[ci - index_step, cr, cc] > threshold_hu:\n",
    "                    index_step += 1\n",
    "            except IndexError: #indexError is raised when we reach the end of the axis\n",
    "                index_step -= 1\n",
    "\n",
    "            row_step = 2\n",
    "            try:\n",
    "                while self.ct_img_arr[ci, cr + row_step, cc] > threshold_hu and \\\n",
    "                        self.ct_img_arr[ci, cr - row_step, cc] > threshold_hu:\n",
    "                    row_step += 1\n",
    "            except IndexError:\n",
    "                row_step -= 1\n",
    "\n",
    "            col_step = 2\n",
    "            try:\n",
    "                while self.ct_img_arr[ci, cr, cc + col_step] > threshold_hu and \\\n",
    "                        self.ct_img_arr[ci, cr, cc - col_step] > threshold_hu:\n",
    "                    col_step += 1\n",
    "            except IndexError:\n",
    "                col_step -= 1\n",
    "\n",
    "            \"\"\"mask_arr is a 3d tensor of the same size as the ct, and that is False everywhere except where the candidate is located,\"\"\"\n",
    "            mask_arr[ \n",
    "                ci - index_step: ci + index_step + 1,\n",
    "                cr - row_step: cr + row_step + 1,\n",
    "                cc - col_step: cc + col_step + 1] = True\n",
    "        # need to do clean up because we stop expanding the bounding box when we reach the # threshold_hu without decreasing the increased step\n",
    "        # filter out the low density boxes that are bordering the high density boxes\n",
    "        mask_arr = mask_arr & (self.ct_img_arr > threshold_hu) # clean up bordering low density boxes\n",
    "        \n",
    "        \"\"\"we return the full mask, and we do the cropping on both the CT scan and the mask when we actually get the candidate chunk. (in ct.get_ct_cropped method)\"\"\"\n",
    "        return mask_arr       \n",
    "\n",
    "    @staticmethod\n",
    "    @disk_cache.memoize(typed=True) \n",
    "    #cache this for fast retrieval of Index axis size and pos idxs\n",
    "    # we need the Index axis size because each ct scan has different number of slices\n",
    "    def get_Ct_I_axis_info(series_uid):\n",
    "        ct = CTScan_seg(series_uid)\n",
    "        return int(ct.ct_img_arr.shape[0]), ct.positive_indexes\n",
    "\n",
    "\n",
    "class LunaSegDataset(Dataset):\n",
    "    #custome implementation of a dataset that loads the CT scans and candidate info\n",
    "    \n",
    "    series_uids_list = get_series_uids_list()\n",
    "    df_candidates = create_df_candidates_info()\n",
    "    split_idx = math.ceil(len(series_uids_list) * 0.7)\n",
    "    # dataloader probably does shallow copy of the object when numworkers > 0\n",
    "    # so df_candidates must stay outside of __init__ for it to be copied to each worker\n",
    "    def __init__(self) -> None:\n",
    "        self.samples = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     pass\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# training and validation datasets classes, which share the same parent self.df_candidates, \n",
    "# but the shared df_candidates is splitted\n",
    "class LunaSegDataset_Train(LunaSegDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.series_uids_list = self.series_uids_list[:self.split_idx]\n",
    "        df_candidates_reset_index = self.df_candidates.reset_index()\n",
    "        self.samples = df_candidates_reset_index[(df_candidates_reset_index['seriesuid']\n",
    "                                            .isin(self.series_uids_list)) \n",
    "                                           & (df_candidates_reset_index['isNodule'] == True)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        candidate_info = self.samples.iloc[idx]\n",
    "        ct_arr, pos_arr = CTScan_seg.get_ct_cropped_disk_cache(\n",
    "            candidate_info['seriesuid'],\n",
    "            candidate_info['xyzCoord'],\n",
    "            (7, 96, 96)\n",
    "            )\n",
    "        ct_tensor = torch.tensor(ct_arr, dtype=torch.float32)\n",
    "        # we will use conv2d, so no need to add the channel dimension\n",
    "        # because we treat the I axis as the channel dimension already\n",
    "        # and there is no depth D dimension\n",
    "        pos_tensor = torch.tensor(pos_arr, dtype=torch.long)\n",
    "        return ct_tensor, pos_tensor\n",
    "        \n",
    "class LunaSegDataset_Val(LunaSegDataset):\n",
    "    def __init__(self):\n",
    "        self.series_uids_list = self.series_uids_list[self.split_idx:]\n",
    "        self.samples = []\n",
    "        for series_uid in self.series_uids_list:\n",
    "            I_count, pos_idxs = CTScan_seg.get_Ct_I_axis_info(series_uid)\n",
    "            self.samples.extend([(series_uid, pos_idx) for pos_idx in pos_idxs])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        series_uid, slice_idx = self.samples[idx]\n",
    "        ct = CTScan_seg._get_single_ct_lru_cache(series_uid)\n",
    "        ct_t = torch.zeros((3*2 + 1, 512, 512))\n",
    "        \n",
    "        #for every 2d input slice, we accompany it with 3 slices before and after it\n",
    "        start_idx = slice_idx - 3\n",
    "        end_idx = slice_idx + 3 + 1\n",
    "        for i, context_idx in enumerate(range(start_idx, end_idx)):\n",
    "            #When we reach beyond the bounds of the ct_a, we duplicate the first or last slice.\n",
    "            context_idx = max(context_idx, 0)\n",
    "            context_idx = min(context_idx, ct.ct_img_arr.shape[0] - 1)\n",
    "            ct_t[i] = torch.from_numpy(ct.ct_img_arr[context_idx].astype(np.float32))\n",
    "\n",
    "        ct_t.clamp_(-1000, 1000) # have not done the clipping\n",
    "\n",
    "        assert len(ct.positive_mask) > 0, f\"positive mask is empty for {series_uid}\"\n",
    "        pos_t = torch.from_numpy(ct.positive_mask[slice_idx]).unsqueeze(0)\n",
    "\n",
    "        return ct_t, pos_t\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/27 06:25:05 WARNING  util.utilG:044:enumerateWithEstimate cache val ----/4, starting\n",
      "2024/04/27 06:25:14 WARNING  util.utilG:074:enumerateWithEstimate cache val ----/4, done at 2024-04-27 06:25:14\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from util.utilG import enumerateWithEstimate\n",
    "train_set = LunaSegDataset_Train()\n",
    "val_set = LunaSegDataset_Val()\n",
    "train_loader = DataLoader(train_set, batch_size=64)\n",
    "val_loader = DataLoader(val_set, batch_size=64)\n",
    "numworkers = 0\n",
    "\n",
    "# batch_iter = enumerateWithEstimate(\n",
    "#     train_loader,\n",
    "#     \"cache train\",\n",
    "#     start_ndx=numworkers,\n",
    "# )\n",
    "# for batch_ndx, batch_tup in batch_iter:\n",
    "#     pass\n",
    "\n",
    "\n",
    "batch_iter = enumerateWithEstimate(\n",
    "    val_loader,\n",
    "    \"cache val\",\n",
    "    start_ndx=numworkers,\n",
    ")\n",
    "for batch_ndx, batch_tup in batch_iter:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
