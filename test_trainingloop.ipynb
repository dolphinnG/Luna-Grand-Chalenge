{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import logging\n",
    "# import os\n",
    "\n",
    "# import numpy as np\n",
    "# from dsetsG import LunaDataset, LunaDataset_Train, LunaDataset_Val\n",
    "# from test.model import LunaModel\n",
    "# import torch\n",
    "# from  torch.utils.data import DataLoader\n",
    "# from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "# from util.utilG import enumerateWithEstimate\n",
    "\n",
    "# log = logging.getLogger('TrainingLoop')\n",
    "# # log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.INFO)\n",
    "# # log.setLevel(logging.DEBUG)\n",
    "# Y_TRUE = 0\n",
    "# Y_PRED = 1\n",
    "# LOSS = 2\n",
    "# class TrainingLoop:\n",
    "#     def __init__(self, model, optimizer, loss_fn, train_loader, val_loader):\n",
    "#         self.model = model\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.train_loader = train_loader\n",
    "#         self.val_loader = val_loader\n",
    "#         self.use_cuda = torch.cuda.is_available()\n",
    "#         self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "#         self.trn_writer = None\n",
    "#         self.val_writer = None\n",
    "        \n",
    "    \n",
    "#     def run(self, epochs=1):\n",
    "        \n",
    "#         self.model.to(self.device)\n",
    "            \n",
    "#         for epoch_idx in range(1, epochs + 1):\n",
    "            \n",
    "#             #training\n",
    "#             epoch_metrics_tensor_train = torch.zeros(3, len(self.train_loader.dataset)) \n",
    "#             # we want the number of samples, not the number of batches\n",
    "#             self.model.train()\n",
    "#             for batch in enumerateWithEstimate(self.train_loader, f\"EPOCH_train: {epoch_idx}\"):\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss_scalar = self.compute_batch_loss_and_populate_metrics(batch, epoch_metrics_tensor_train)\n",
    "#                 loss_scalar.backward()\n",
    "#                 self.optimizer.step()\n",
    "#             self.logMetrics(epoch_idx, 'trn', epoch_metrics_tensor_train)\n",
    "            \n",
    "#             #validation\n",
    "#             epoch_metrics_tensor_val = torch.zeros(3, len(self.val_loader.dataset))\n",
    "#             self.model.eval()\n",
    "#             for batch in enumerateWithEstimate(self.val_loader, f\"EPOCH_val: {epoch_idx}\"):\n",
    "#                 with torch.no_grad():\n",
    "#                     self.compute_batch_loss_and_populate_metrics(batch, epoch_metrics_tensor_val)\n",
    "#             self.logMetrics(epoch_idx, 'val', epoch_metrics_tensor_val)             \n",
    "                    \n",
    "#         if hasattr(self, 'trn_writer'):\n",
    "#             assert self.trn_writer and self.val_writer\n",
    "#             self.trn_writer.close()\n",
    "#             self.val_writer.close()\n",
    "    \n",
    "#     def compute_batch_loss_and_populate_metrics(self, batch, epoch_metrics_tensor):\n",
    "#         batch_idx, batch_data = batch\n",
    "#         inputs, y_true = batch_data\n",
    "        \n",
    "#         y_pred_logits, y_pred_prob = self.model(inputs.to(self.device)) \n",
    "#         # loss_tensor = self.loss_fn(y_pred_logits, y_true[:, 1]) #could have used nn.NLLLoss() \n",
    "#         loss_tensor = self.loss_fn(y_pred_logits, y_true.to(self.device)) #could have used nn.NLLLoss() \n",
    "        \n",
    "#         slice_start = batch_idx * self.train_loader.batch_size\n",
    "#         slice_end = slice_start + len(y_true)\n",
    "#         epoch_metrics_tensor[LOSS, slice_start:slice_end] = loss_tensor.detach()\n",
    "#         # epoch_metrics_tensor[Y_TRUE, slice_start:slice_end] = y_true[:, 1].detach()\n",
    "#         epoch_metrics_tensor[Y_TRUE, slice_start:slice_end] = y_true.detach()\n",
    "#         epoch_metrics_tensor[Y_PRED, slice_start:slice_end] = y_pred_prob[:, 1].detach()\n",
    "        \n",
    "#         return loss_tensor.mean() # reduce to scalar\n",
    "    \n",
    "#     def logMetrics( # log metrics for each epoch\n",
    "#             self,\n",
    "#             epoch_ndx,\n",
    "#             mode_str,\n",
    "#             metrics_t,\n",
    "#             classificationThreshold=0.5,\n",
    "#     ):\n",
    "#         if self.trn_writer is None:\n",
    "#             log_dir = os.path.join('runs', 'Nodule_Classifier', \n",
    "#                                    datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S'))\n",
    "\n",
    "#             self.trn_writer = SummaryWriter(\n",
    "#                 log_dir=log_dir + '-trn_cls-' + 'Nodule_Classifier')\n",
    "#             self.val_writer = SummaryWriter(\n",
    "#                 log_dir=log_dir + '-val_cls-' + 'Nodule_Classifier')\n",
    "#         log.info(\"E{} {}\".format(\n",
    "#             epoch_ndx,\n",
    "#             type(self).__name__,\n",
    "#         ))\n",
    "\n",
    "#         negLabel_mask = metrics_t[Y_TRUE] <= classificationThreshold #groundtruth negative\n",
    "#         negPred_mask = metrics_t[Y_PRED] <= classificationThreshold #prediction negative\n",
    "\n",
    "#         posLabel_mask = ~negLabel_mask\n",
    "#         posPred_mask = ~negPred_mask\n",
    "\n",
    "#         neg_count = int(negLabel_mask.sum())\n",
    "#         pos_count = int(posLabel_mask.sum())\n",
    "\n",
    "#         neg_correct = trueNeg = int((negLabel_mask & negPred_mask).sum()) #true negatives\n",
    "#         pos_correct = truePos = int((posLabel_mask & posPred_mask).sum()) #true positives\n",
    "\n",
    "#         falsePos = neg_count - trueNeg\n",
    "#         falseNeg = pos_count - truePos\n",
    "        \n",
    "#         metrics_dict = {}\n",
    "#         metrics_dict['loss_all'] = \\\n",
    "#             metrics_t[LOSS].mean() #avg loss per epoch of all batches\n",
    "#         metrics_dict['loss_negClass'] = \\\n",
    "#             metrics_t[LOSS, negLabel_mask].mean() #avg loss per epoch of negative class of all batches\n",
    "#         metrics_dict['loss_posClass'] = \\\n",
    "#             metrics_t[LOSS, posLabel_mask].mean() #avg loss per epoch of positive class of all batches\n",
    "\n",
    "#         # metrics_dict['correct/all'] = (pos_correct + neg_correct) \\\n",
    "#         #     / np.float32(metrics_t.shape[1]) * 100 # all correct predictions / all predictions\n",
    "#         metrics_dict['correct_all_accuracy'] = (truePos + trueNeg) / np.float32(pos_count + neg_count) * 100\n",
    "#         # metrics_dict['correct/neg'] = trueNeg / np.float32(neg_count) * 100 # true negatives / all actual negatives = specificity = true nagative rate\n",
    "#         metrics_dict['specificity'] = trueNeg / np.float32(trueNeg + falsePos) * 100 #true nagative rate\n",
    "#         # metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100 # true positives / all actual positives = recall = true positive rate\n",
    "#         metrics_dict['recall'] = truePos / np.float32(truePos + falseNeg) \n",
    "#         metrics_dict['recall_pct'] = metrics_dict['recall'] * 100\n",
    "    \n",
    "#         metrics_dict['precision'] = truePos / np.float32(truePos + falsePos) \n",
    "#         metrics_dict['f1_score'] = (2 * (metrics_dict['precision'] * metrics_dict['recall']) \n",
    "#                                     / np.float32(metrics_dict['precision'] + metrics_dict['recall']))\n",
    "        \n",
    "        \n",
    "#         log.info(\n",
    "#             (\"E{} {:8} {loss_all:.4f} loss, \"\n",
    "#                   \"{correct_all_accuracy:-5.1f}% accuracy, \"\n",
    "#                   \"{recall:.4f} recall, \"\n",
    "#                   \"{precision:.4f} precision, \"\n",
    "#                     \"{f1_score:.4f} f1_score\"\n",
    "#             ).format(\n",
    "#                 epoch_ndx,\n",
    "#                 mode_str,\n",
    "#                 **metrics_dict,\n",
    "#             )\n",
    "#         )\n",
    "#         log.info(\n",
    "#             (\"E{} {:8} {loss_negClass:.4f} loss, \"\n",
    "#                 \"{specificity:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "#             ).format(\n",
    "#                 epoch_ndx,\n",
    "#                 mode_str + '_neg',\n",
    "#                 neg_correct=trueNeg,\n",
    "#                 neg_count=neg_count,\n",
    "#                 **metrics_dict,\n",
    "#             )\n",
    "#         )\n",
    "#         log.info(\n",
    "#             (\"E{} {:8} {loss_posClass:.4f} loss, \"\n",
    "#                 \"{recall_pct:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "#             ).format(\n",
    "#                 epoch_ndx,\n",
    "#                 mode_str + '_pos',\n",
    "#                 pos_correct=truePos,\n",
    "#                 pos_count=pos_count,\n",
    "#                 **metrics_dict,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         writer:SummaryWriter = getattr(self, mode_str + '_writer')\n",
    "         \n",
    "#         for key, value in metrics_dict.items():\n",
    "#             writer.add_scalar(key, value, epoch_ndx)\n",
    "\n",
    "#         writer.add_pr_curve(\n",
    "#             'pr',\n",
    "#             metrics_t[Y_TRUE],\n",
    "#             metrics_t[Y_PRED],\n",
    "#             epoch_ndx\n",
    "#         )\n",
    "\n",
    "#         bins = [x/50.0 for x in range(51)]\n",
    "\n",
    "#         negHist_mask = negLabel_mask & (metrics_t[Y_PRED] > 0.01)\n",
    "#         posHist_mask = posLabel_mask & (metrics_t[Y_PRED] < 0.99)\n",
    "\n",
    "#         if negHist_mask.any():\n",
    "#             writer.add_histogram(\n",
    "#                 'is_neg',\n",
    "#                 metrics_t[Y_PRED, negHist_mask],\n",
    "#                 epoch_ndx,\n",
    "#                 bins=bins, # type: ignore\n",
    "#             )\n",
    "#         if posHist_mask.any():\n",
    "#             writer.add_histogram(\n",
    "#                 'is_pos',\n",
    "#                 metrics_t[Y_PRED, posHist_mask],\n",
    "#                 epoch_ndx,\n",
    "#                 bins=bins, # type: ignore\n",
    "#             )\n",
    "            \n",
    "# def build_training_loop():\n",
    "#     unzipped_path = 'D:/LIDC-IDRI_unzipped'\n",
    "#     model = LunaModel()\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "#     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "#     batch_size = 64\n",
    "#     num_workers = 4\n",
    "#     # dataset = LunaDataset()\n",
    "#     trainset = LunaDataset_Train()\n",
    "#     valset = LunaDataset_Val()\n",
    "#     # dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "#     train_loader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers)\n",
    "#     val_loader = DataLoader(valset, batch_size=batch_size, num_workers=num_workers)\n",
    "#     return TrainingLoop(model, optimizer, loss_fn, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/27 04:03:21 WARNING  util.utilG:044:enumerateWithEstimate EPOCH_train: 1 ----/609, starting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justm\\OneDrive\\Desktop\\Luna Grand Chalenge\\test_trainingloop.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justm/OneDrive/Desktop/Luna%20Grand%20Chalenge/test_trainingloop.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnodule_classifier\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining_loop\u001b[39;00m \u001b[39mimport\u001b[39;00m build_training_loop\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justm/OneDrive/Desktop/Luna%20Grand%20Chalenge/test_trainingloop.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m loop \u001b[39m=\u001b[39m build_training_loop()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/justm/OneDrive/Desktop/Luna%20Grand%20Chalenge/test_trainingloop.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loop\u001b[39m.\u001b[39;49mrun(epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\justm\\OneDrive\\Desktop\\Luna Grand Chalenge\\nodule_classifier\\training_loop.py:44\u001b[0m, in \u001b[0;36mTrainingLoop.run\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m# we want the number of samples, not the number of batches\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 44\u001b[0m \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m enumerateWithEstimate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEPOCH_train: \u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch_idx\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m     45\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     46\u001b[0m     loss_scalar \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_batch_loss_and_populate_metrics(batch, epoch_metrics_tensor_train)\n",
      "File \u001b[1;32mc:\\Users\\justm\\OneDrive\\Desktop\\Luna Grand Chalenge\\util\\utilG.py:49\u001b[0m, in \u001b[0;36menumerateWithEstimate\u001b[1;34m(iter, loop_title, start_ndx, print_ndx, jump, iter_len)\u001b[0m\n\u001b[0;32m     44\u001b[0m log\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m ----/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, starting\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     45\u001b[0m     loop_title,\n\u001b[0;32m     46\u001b[0m     iter_len,\n\u001b[0;32m     47\u001b[0m ))\n\u001b[0;32m     48\u001b[0m start_ts \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 49\u001b[0m \u001b[39mfor\u001b[39;00m (current_ndx, item) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(\u001b[39miter\u001b[39;49m):\n\u001b[0;32m     50\u001b[0m     \u001b[39myield\u001b[39;00m (current_ndx, item)\n\u001b[0;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m current_ndx \u001b[39m==\u001b[39m print_ndx:\n\u001b[0;32m     52\u001b[0m         \u001b[39m# ... <1>\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\justm\\OneDrive\\Desktop\\Luna Grand Chalenge\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\justm\\OneDrive\\Desktop\\Luna Grand Chalenge\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\justm\\OneDrive\\Desktop\\Luna Grand Chalenge\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     96\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nodule_classifier.training_loop import build_training_loop\n",
    "\n",
    "\n",
    "loop = build_training_loop()\n",
    "loop.run(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
