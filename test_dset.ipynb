{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "unzipped_path = 'C:\\\\Users\\\\justm\\\\OneDrive\\\\Desktop\\\\New folder\\\\unzipped\\\\'\n",
    "\n",
    "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
    "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
    "\n",
    "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_matrix):\n",
    "    cri_a = coord_irc[::-1]\n",
    "    # cri_a = np.array(coord_irc)[::-1]\n",
    "    # origin_a = np.array(origin_xyz)\n",
    "    # vxSize_a = np.array(vxSize_xyz)\n",
    "    coords_xyz = (direction_matrix @ (cri_a * vxSize_xyz)) + origin_xyz\n",
    "    return coords_xyz\n",
    "\n",
    "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_matrix):\n",
    "    # origin_a = np.array(origin_xyz)\n",
    "    # vxSize_a = np.array(vxSize_xyz)\n",
    "    # coord_a = np.array(coord_xyz)\n",
    "    cri_a = ((coord_xyz - origin_xyz) @ np.linalg.inv(direction_matrix)) / vxSize_xyz\n",
    "    return np.round(cri_a).astype(int)[::-1]\n",
    "    # return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "            unzipped_path + 'subset*/subset*/{}.mhd'.format(series_uid)\n",
    "        )[0]\n",
    "\n",
    "        ct_metadata = sitk.ReadImage(mhd_path)\n",
    "        # ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "        ct_arr = sitk.GetArrayFromImage(ct_metadata).astype(np.float32)\n",
    "        #crop HU values to [-1000, 1000]\n",
    "        ct_arr.clip(-1000, 1000, ct_arr)\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_arr\n",
    "\n",
    "        self.origin_xyz = np.array(ct_metadata.GetOrigin())\n",
    "        self.vxSize_xyz = np.array(ct_metadata.GetSpacing())\n",
    "        self.direction_a = np.array(ct_metadata.GetDirection()).reshape(3, 3)\n",
    "        \n",
    "    def get_candidate_croppedChunk_inVoxelCoord(self, center_xyz, width_irc = (32, 48, 48)):\n",
    "        center_irc = xyz2irc(\n",
    "            np.array(center_xyz),\n",
    "            self.origin_xyz,\n",
    "            self.vxSize_xyz,\n",
    "            self.direction_a,\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
    "            # assert check irc center is within the CT array\n",
    "            \n",
    "            if start_ndx < 0:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                start_ndx = 0\n",
    "                end_ndx = int(width_irc[axis])\n",
    "                # if start_ndx < 0, set start_ndx to 0 and end_ndx to width_irc[axis]\n",
    "\n",
    "            if end_ndx > self.hu_a.shape[axis]:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                end_ndx = self.hu_a.shape[axis]\n",
    "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
    "                # if end_ndx > axis size, set end_ndx to axis size and start_ndx to axis size - width_irc[axis]\n",
    "\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ct('1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ct scan instance class:\n",
    "\n",
    "xyz <-> irc #irc needs to be flipped the order to cri\n",
    "\n",
    "sitk(seriesuid) read image into array\n",
    "clip HU values [-1000, 1000]\n",
    "lru cache single ct instance\n",
    "    # that's why need lru cache of size 1, \n",
    "    # but we shuffled the candidateinfolist, size 1 cache is not enough\n",
    "    # so ct object cache is only useful during prepcache, the dset always rely on diskcached ct chunks\n",
    "disk cache cropped ct chunks = function get_candidate_croppedChunk_inVoxelCoord\n",
    "crop the ct to the size of a width_irc tuple, each axis is to be (axisCoord +- width_irc[axis]/2)\n",
    "# assert check irc center is within the CT array\n",
    "# if start_ndx < 0, set start_ndx to 0 and end_ndx to width_irc[axis]\n",
    "# if end_ndx > axis size, set end_ndx to axis size and start_ndx to axis size - width_irc[axis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "from utilG import getCacheHandle, unzipped_path\n",
    "\n",
    "\n",
    "disk_cache = getCacheHandle('test1')\n",
    "\n",
    "class CTScan:\n",
    "    def __init__(self, seriesuid) -> None:\n",
    "        self.seriesuid = seriesuid\n",
    "        path_mhdfile = glob.glob(unzipped_path + 'subset*/subset*/{}.mhd'.format(seriesuid))[0]\n",
    "        ct_img = sitk.ReadImage(path_mhdfile) #contain metadata getters\n",
    "        ct_img_arr = sitk.GetArrayFromImage(ct_img).astype(np.float32)\n",
    "        self.ct_img_arr = ct_img_arr\n",
    "        #crop HU values to [-1000, 1000]\n",
    "        np.clip(ct_img_arr, -1000, 1000, out=ct_img_arr)\n",
    "        \n",
    "        self.origin_xyz = np.array(ct_img.GetOrigin())\n",
    "        self.vxSize_xyz = np.array(ct_img.GetSpacing())\n",
    "        self.direction_matrix = np.array(ct_img.GetDirection()).reshape(3, 3)\n",
    "    \n",
    "    def get_candidate_croppedChunk_inVoxelCoord(self, center_xyz, axis_sizes = (32, 48, 48)):\n",
    "        \"\"\"\n",
    "        center_xyz: tuple of 3 floats, center of the chunk in xyz coord\n",
    "        axis_size: tuple of 3 integers, size of the chunk in each axis. Default is (32, 48, 48)\n",
    "        \"\"\"\n",
    "        center_irc = xyz2irc(np.array(center_xyz), self.origin_xyz, self.vxSize_xyz, self.direction_matrix)\n",
    "        slices = []\n",
    "        for idx, axis_size in enumerate(axis_sizes):\n",
    "            start_idx = int(round(center_irc[idx] - axis_size/2))\n",
    "            end_idx = int(start_idx + axis_size)\n",
    "\n",
    "            # if start_idx < 0, set start_idx to 0 and end_idx to axis_size[axis]\n",
    "            # if end_idx > ct_sizes[axis], set end_idx to axis_size and start_idx to ct_sizes[axis] - axis_size\n",
    "            ct_sizes = self.ct_img_arr.shape\n",
    "            \n",
    "            if start_idx < 0:\n",
    "                start_idx = 0\n",
    "                end_idx = axis_size\n",
    "            if end_idx > ct_sizes[idx]:\n",
    "                end_idx = ct_sizes[idx]\n",
    "                start_idx = int(ct_sizes[idx] - axis_size)\n",
    "            \n",
    "            slices.append(slice(start_idx, end_idx))\n",
    "        ct_cropped = self.ct_img_arr[tuple(slices)]\n",
    "        return ct_cropped\n",
    "\n",
    "@lru_cache(maxsize=1, typed=True)\n",
    "def get_single_ct_lru_cache(seriesuid):\n",
    "    ct = CTScan(seriesuid)\n",
    "    return ct\n",
    "\n",
    "@disk_cache.memoize(typed=True)\n",
    "def get_ct_cropped_disk_cache(seriesuid, center_xyz, axis_sizes = (32, 48, 48)):\n",
    "    ct = get_single_ct_lru_cache(seriesuid)\n",
    "    # this is why need lru cache of size 1, \n",
    "    # but we shuffled the dataset, size 1 cache is not enough\n",
    "    # so single ct object cache is only useful during prepcache, the datasets always rely on diskcached ct chunks\n",
    "    return ct.get_candidate_croppedChunk_inVoxelCoord(center_xyz, axis_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CTScan at 0x1807eb2f590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ctG = CTScan('1.3.6.1.4.1.14519.5.2.1.6279.6001.625270601160880745954773142570')\n",
    "# ct = Ct('1.3.6.1.4.1.14519.5.2.1.6279.6001.625270601160880745954773142570')\n",
    "# ctG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (ctG.get_candidate_croppedChunk_inVoxelCoord((-108.2007072, 46.48017452, -143.2481594)) == ct.get_candidate_croppedChunk_inVoxelCoord((-108.2007072, 46.48017452, -143.2481594))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55603 38922 16681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 38922, 16681)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from test.util.util import enumerateWithEstimate\n",
    "from dsetsG import LunaDataset, LunaDataset_Train, LunaDataset_Val\n",
    "\n",
    "\n",
    "dataset = LunaDataset()\n",
    "trainset = LunaDataset_Train()\n",
    "valset = LunaDataset_Val()\n",
    "print(len(dataset), len(trainset), len(valset))\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=num_workers)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "duplicate_rows = pd.merge(trainset.df_candidates, valset.df_candidates, how='inner')\n",
    "assert len(duplicate_rows) == 0, \"duplicate rows in train and val set\"\n",
    "len(duplicate_rows), len(trainset.df_candidates), len(valset.df_candidates)\n",
    "\n",
    "# batch_iter = enumerateWithEstimate(dataloader,\"test cache 1\", start_ndx=num_workers, jump = 4)\n",
    "# for _ in batch_iter:\n",
    "#     pass\n",
    "# batch_iter = enumerateWithEstimate(trainloader,\"test cache 2\", start_ndx=num_workers,)\n",
    "# for _ in batch_iter:\n",
    "#     pass\n",
    "# batch_iter = enumerateWithEstimate(valloader,\"test cache 3\", start_ndx=num_workers,)\n",
    "# for _ in batch_iter:\n",
    "#     pass\n",
    "# dataset.df_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hehe\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self) -> None:\n",
    "        A.a = 'hehe'\n",
    "    def test(self):\n",
    "        print(self.a)\n",
    "A().test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# clean up any old data that might be around.\n",
    "# We don't call this by default because it's destructive, \n",
    "# and would waste a lot of time if it ran when nothing \n",
    "# on the application side had changed.\n",
    "def cleanCache():\n",
    "    shutil.rmtree('C:\\\\Users\\\\justm\\\\OneDrive\\\\Desktop\\\\Luna Grand Chalenge\\\\disk_cache')\n",
    "    os.mkdir('C:\\\\Users\\\\justm\\\\OneDrive\\\\Desktop\\\\Luna Grand Chalenge\\\\disk_cache')\n",
    "\n",
    "# cleanCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
